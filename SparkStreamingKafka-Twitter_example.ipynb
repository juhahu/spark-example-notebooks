{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming and processing Twitter data from Kafka topic using Spark Streaming\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created using https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/ blogpost as an inspiration.\n",
    "\n",
    "You need Kafka cluster and producer that is reading Twitter API and storing tweets to Kafka topic twitter-tweets. Kafka cluster have to be accessible from the Spark cluster running this notebook. This demonstration is done using CSC Rahti (OpenShift) environment, running Apache Spark version 2.4.0 and Strimzi Kafka cluster in same namespaces.\n",
    "https://strimzi.io/2018/05/17/running-strimzi-on-openshift-online.html\n",
    "\n",
    "More info about integrating Kafka to Spark Streaming can be found from https://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html\n",
    "and https://spark.apache.org/docs/2.3.1/api/python/pyspark.streaming.html#module-pyspark.streaming.kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setting up system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:32.735433Z",
     "start_time": "2019-07-04T10:29:32.728891Z"
    }
   },
   "outputs": [],
   "source": [
    "# First include Maven repository for Spark Streaming Kafka\n",
    "# Check your Spark version and get correct version of repository\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:34.315293Z",
     "start_time": "2019-07-04T10:29:33.773794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spark, Spark Streaming, Kafka and json for twitter data\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:35.641390Z",
     "start_time": "2019-07-04T10:29:35.635666Z"
    }
   },
   "outputs": [],
   "source": [
    "# specify Kafka variables\n",
    "zookeeper = 'my-cluster-zookeeper:2181' # Zookeeper server default address and port for Strimzi cluster\n",
    "group = 'spark-streaming' # Consumer group name, can be defined here\n",
    "topic = 'twitter-tweets' # Kafkat topic to be consumed \n",
    "partitions = 2 # Each partition is consumed in its own thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:50.820977Z",
     "start_time": "2019-07-04T10:29:37.438790Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Twitter Streaming from Kafka\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "# Initialize streaming Context and define processing interval in seconds\n",
    "ssc = StreamingContext(sc, 30)\n",
    "kafkaStream = KafkaUtils.createStream(ssc, zookeeper, group, {topic:partitions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark commands to be run on streming context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting tweets per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:51.036147Z",
     "start_time": "2019-07-04T10:29:50.825333Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed = kafkaStream.map(lambda v: json.loads(v[1]))\n",
    "parsed.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()\n",
    "authors_dstream = parsed.map(lambda tweet: tweet['user']['screen_name'])\n",
    "author_counts = authors_dstream.countByValue()\n",
    "author_counts.pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting users according their productivity and printing top five authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:52.119400Z",
     "start_time": "2019-07-04T10:29:52.094920Z"
    }
   },
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream = author_counts.transform(\\\n",
    "  (lambda foo:foo\\\n",
    "   .sortBy(lambda x:( -x[1]))))\n",
    "\n",
    "top_five_authors = author_counts_sorted_dstream.transform\\\n",
    "  (lambda rdd:sc.parallelize(rdd.take(5)))\n",
    "top_five_authors.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing text and counting individual words and printing sorted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:29:55.146160Z",
     "start_time": "2019-07-04T10:29:55.130103Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed.\\\n",
    "    flatMap(lambda tweet:tweet['text'].split(\" \"))\\\n",
    "    .countByValue()\\\n",
    "    .transform\\\n",
    "      (lambda rdd:rdd.sortBy(lambda x:-x[1]))\\\n",
    "    .pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting streaming context to be run on 30 sec interval\n",
    "\n",
    "It'll take 30 seconds before anything happens so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T10:31:19.377362Z",
     "start_time": "2019-07-04T10:30:31.296168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-07-04 12:31:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 565\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-04 12:31:00\n",
      "-------------------------------------------\n",
      "('User1', 1)\n",
      "('User2', 1)\n",
      "('User3', 1)\n",
      "('User4', 1)\n",
      "('User5', 1)\n",
      "('User6', 1)\n",
      "('User7', 1)\n",
      "('User8', 1)\n",
      "('User9', 1)\n",
      "('User10', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-04 12:31:00\n",
      "-------------------------------------------\n",
      "('UserX', 3)\n",
      "('UserX', 2)\n",
      "('UserX', 2)\n",
      "('UserX', 2)\n",
      "('UserX', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-07-04 12:31:00\n",
      "-------------------------------------------\n",
      "('RT', 353)\n",
      "('and', 176)\n",
      "('the', 155)\n",
      "('in', 131)\n",
      "('to', 121)\n",
      "('on', 100)\n",
      "('a', 93)\n",
      "('of', 92)\n",
      "('you', 91)\n",
      "('this', 88)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
