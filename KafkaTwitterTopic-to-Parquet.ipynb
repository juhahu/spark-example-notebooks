{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch job to read Kafka topic to Spark and save it to Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to read and save data from Kafka Topic to prevent data loss after Kafka 7 day retention period.\n",
    "\n",
    "Some cleaning to data is also done and user can modify what data to save. Data in Kafka topic is in JSON format. Twitter JSON structure can be studied from [Twitter Developer pages](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json.html) and fields to save can be specified later in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces also some basics of Apache Spark so there are some extra data printing and schema manipulation presented with explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:46:18.305634Z",
     "start_time": "2019-07-23T13:46:18.299104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import spark sql kafka package from Maven repository\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:46:19.961694Z",
     "start_time": "2019-07-23T13:46:19.483754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary functions to use Spark SQL and json format\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType , LongType, StringType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:46:36.886463Z",
     "start_time": "2019-07-23T13:46:21.171132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka topic to Parquet\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:46:57.758865Z",
     "start_time": "2019-07-23T13:46:53.939326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create batch job to subscribe to the topic twitter-tweets and get all data from it (between the earliest and the latest offsets) \n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"twitter-tweets\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Some data exploration to understand why we need to alter schema and do datatype transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:47:18.459191Z",
     "start_time": "2019-07-23T13:46:58.387475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting tweets\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:47:20.684727Z",
     "start_time": "2019-07-23T13:47:18.463760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "| key|               value|         topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 63 72 65 6...|twitter-tweets|        2|     0|2019-07-22 13:28:...|            0|\n",
      "|null|[7B 22 63 72 65 6...|twitter-tweets|        2|     1|2019-07-22 13:28:...|            0|\n",
      "|null|[7B 22 63 72 65 6...|twitter-tweets|        2|     2|2019-07-22 13:28:...|            0|\n",
      "|null|[7B 22 63 72 65 6...|twitter-tweets|        2|     3|2019-07-22 13:28:...|            0|\n",
      "|null|[7B 22 63 72 65 6...|twitter-tweets|        2|     4|2019-07-22 13:28:...|            0|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample of content and schema of data\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Kafka topic data includes tweets in 'value' column in binary format.\n",
    "We want to take just that 'value' column and change data type to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:47:22.667582Z",
     "start_time": "2019-07-23T13:47:22.103501Z"
    }
   },
   "outputs": [],
   "source": [
    "jsonDF = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from 'value' column is now string in 'jsonDF' DataFrame and because Twitter data is JSON we need to extract that JSON and get what data we want from it.\n",
    "\n",
    "**If you want to save more fields, consult [Twitter Developer pages](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json.html) API documentation and modify schema below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:47:25.766299Z",
     "start_time": "2019-07-23T13:47:25.535225Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet: struct (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First create schema of our data with proper datatypes and then convert dataframe to use that schema\n",
    "schema = StructType([StructField('created_at', StringType(), True),\n",
    "                     StructField('id', LongType(), True),\n",
    "                     StructField('user', StructType([StructField('id', LongType(), True),\n",
    "                                         StructField('name', StringType(), True),\n",
    "                                         StructField('screen_name', StringType(), True)]), True),\n",
    "                     StructField('text', StringType(), True)])\n",
    "\n",
    "tweetNestedDF = jsonDF.select(from_json(col(\"value\"), schema).alias(\"tweet\"))\n",
    "\n",
    "tweetNestedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:47:31.157788Z",
     "start_time": "2019-07-23T13:47:31.100541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's flatten the structure and change also some column names to avoid confusion with two different id column\n",
    "tweetFlattenedDf = tweetNestedDF.selectExpr(\"tweet.created_at\", \"tweet.id as tweet_id\", \"tweet.user.id as user_id\", \"tweet.user.name as user_name\", \"tweet.user.screen_name\", \"tweet.text\")\n",
    "\n",
    "tweetFlattenedDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:45.889410Z",
     "start_time": "2019-07-23T13:42:43.460200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show first 5 items from dataframe\n",
    "tweetFlattenedDf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data to Parquet file. Kafka retention period is only 7 days by default so data should be saved for later use. Please note that write will fail if file already exists. Change file name if you run this repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.391209Z",
     "start_time": "2019-07-23T13:42:45.893350Z"
    }
   },
   "outputs": [],
   "source": [
    "tweetFlattenedDf.write.parquet(\"data/tweets.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
